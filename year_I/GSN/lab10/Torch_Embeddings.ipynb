{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4vWfQx7rtmxg"
   },
   "source": [
    "# Reuters 20 Newsgroups classification\n",
    "\n",
    "In this exercise we will build a classifier for 20 Newsgroups classification. In this task, your aim is to build a classifier which will classifiy a given content to one of 20 classes. Let us start from loading the data. We will use `sklearn` module to download it and preprocess it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M9SpPV84u6ap"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets as sk_datasets\n",
    "\n",
    "train_data_raw = sk_datasets.fetch_20newsgroups(\n",
    "    subset='train',\n",
    ")\n",
    "test_data_raw = sk_datasets.fetch_20newsgroups(\n",
    "    subset='test',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NdDg-dUI1TIx"
   },
   "source": [
    "Raw data is a dictionary with a lot of data which is actually needed for this exercise. We will concentrate only on texts and targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ru0Yn9ZKvHUc"
   },
   "outputs": [],
   "source": [
    "train_texts_raw, train_y = train_data_raw['data'], train_data_raw['target']\n",
    "test_texts_raw, test_y = test_data_raw['data'], test_data_raw['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a4YKSCXM1zJ1"
   },
   "source": [
    "Let us have a look at an example text from training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G7QrBSNJxD3y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_texts_raw[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fA9rxwHB2AYM"
   },
   "source": [
    "As you may see, there is a lot of additional stuff in this text. Now, let us concentrate on cleaning up the data. In practical applications, cleaning up and understanding potential flaws of your texts is often crucial. In the next cell, try to implement a cleaning up function. We expect from you that it will do the following three things:\n",
    "- Remove the header of the data,\n",
    "- Delete all `\\n` characters,\n",
    "- [Optional] Delete the footer.\n",
    "\n",
    "Feel invited to look for more potential clean ups. Share them on `Slack`. Let us compare how different ways of preprocessing data might affect your models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dvu8zG3W1s2L"
   },
   "outputs": [],
   "source": [
    "def clean_up_text(text):\n",
    "    text = ' '.join(text.split('\\n\\n')[1:]) \n",
    "    return text\n",
    "  \n",
    "  \n",
    "def clean_up_texts(texts):\n",
    "    return [clean_up_text(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gnzX0FLg3U6D"
   },
   "source": [
    "Now - let us apply the clean up function to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9k756ylw3UIj"
   },
   "outputs": [],
   "source": [
    "train_texts = clean_up_texts(train_texts_raw)\n",
    "test_texts = clean_up_texts(test_texts_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0lk95hKC_E2G"
   },
   "source": [
    "Now we are in a really interesting moment when a particular representation gap is striking. The collections which were created in a cell above are only collections of lists of characters. Although texts stored there make sense for us, it is only because our brains were trained to read. For a computer, the representation above is equivalent to collection of `int`s. This is where the concept of `embedding` comes in place. We need to represent texts in a form which is able to represent the semantic structure. \n",
    "\n",
    "We will use the [`GloVe`](https://nlp.stanford.edu/projects/glove/) embeddings. Let us download and unzip them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F2SvjGaT5JXS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-05-24 08:51:12--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Translacja nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Łączenie się z nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... połączono.\n",
      "Żądanie HTTP wysłano, oczekiwanie na odpowiedź... 302 Found\n",
      "Lokalizacja: https://nlp.stanford.edu/data/glove.6B.zip [podążanie]\n",
      "--2019-05-24 08:51:13--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Łączenie się z nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... połączono.\n",
      "Żądanie HTTP wysłano, oczekiwanie na odpowiedź... 301 Moved Permanently\n",
      "Lokalizacja: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [podążanie]\n",
      "--2019-05-24 08:51:16--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Translacja downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Łączenie się z downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... połączono.\n",
      "Żądanie HTTP wysłano, oczekiwanie na odpowiedź... 200 OK\n",
      "Długość: 862182613 (822M) [application/zip]\n",
      "Zapis do: `glove.6B.zip'\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822,24M   898KB/s    w 15m 51s  \n",
      "\n",
      "2019-05-24 09:07:09 (886 KB/s) - zapisano `glove.6B.zip' [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T17XESbAnuSi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(os.listdir())\n",
    "\n",
    "!unzip glove.6B.zip.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fFwUP19QYmPM"
   },
   "source": [
    "Now, we can prepare an `Embedding` dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ByBdbLSQEmnp"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "PATH_FOR_50_D_EMBEDDING = 'glove.6B.50d.txt'\n",
    "PATH_FOR_100_D_EMBEDDING = 'glove.6B.100d.txt'\n",
    "PATH_FOR_200_D_EMBEDDING = 'glove.6B.200d.txt'\n",
    "PATH_FOR_300_D_EMBEDDING = 'glove.6B.300d.txt'\n",
    "\n",
    "\n",
    "def get_word_and_vector_from_line(line):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:], dtype='float32')\n",
    "    return word, vector\n",
    "\n",
    "\n",
    "def get_embedding_dict_from_file(filepath=PATH_FOR_100_D_EMBEDDING):\n",
    "    embedding_dict = collections.OrderedDict()\n",
    "    with open(filepath) as glove_file:\n",
    "        for line in glove_file:\n",
    "            word, vector = get_word_and_vector_from_line(line)\n",
    "            embedding_dict[word] = vector\n",
    "    print('Found %s word vectors.' % len(embedding_dict))\n",
    "    return embedding_dict\n",
    "  \n",
    "  \n",
    "embedding_dict = get_embedding_dict_from_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X9WPjIThEnSE"
   },
   "source": [
    "For now, we will use only `100D` embedding. Later, one can build embedding dict for other embeddings (including `50D`, `200D`,`300D`). We are using `OrderedDict`, so accidental operations on dictionary will not change the ordering of words.\n",
    "\n",
    "Ok, the next important step is to prepare the tokenizer tool. The tokenizer should:\n",
    "- remove all unneeded characters (e.g. punctuation, white spaces, etc.),\n",
    "- lower all strings,\n",
    "- split texts into series of words / tokens,\n",
    "- [Optional] perform some sort of text normalization (e.g. stemming or lemmatization),\n",
    "- [Optional] remove stop words from the text (one can use NLTK stop words)\n",
    "\n",
    "Try to think closely about what additional normalization might change in your model performance. If you try to use any normalization on your data, share your results on Slack. Is stemming a good method of string normalization in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EKYgAvYKFpuU"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "  \n",
    "    def tokenize_text(self, text):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def tokenize_texts(self, texts):\n",
    "        return [self.tokenize_text(text) for text in texts]\n",
    "      \n",
    "      \n",
    "class StudentTokenizer(Tokenizer):\n",
    "    pass \n",
    "  \n",
    "  \n",
    "\n",
    "tokenizer = StudentTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5yoBBrKYd6m7"
   },
   "source": [
    "Now, we can tokenize texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hUy5A-b0gNdo"
   },
   "outputs": [],
   "source": [
    "tokenized_train_texts = tokenizer.tokenize_texts(train_texts)\n",
    "tokenized_test_texts = tokenizer.tokenize_texts(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VQXNih9qxedP"
   },
   "source": [
    "Now, let us have a look at how many words are in our embedding dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v418RGwJgVS6"
   },
   "outputs": [],
   "source": [
    "def count_tokens_from_dictionary(tokens, dictionary):\n",
    "    return len(tokens), len([token for token in tokens if token in dictionary])\n",
    "  \n",
    "def flatten_list_of_lists(list_of_lists):\n",
    "    return [el for list_ in list_of_lists for el in list_]\n",
    "  \n",
    "all_train_tokens, dictionary_train_tokens = count_tokens_from_dictionary(\n",
    "    tokens=flatten_list_of_lists(tokenized_train_texts),\n",
    "    dictionary=embedding_dict)\n",
    "all_test_tokens, dictionary_test_tokens = count_tokens_from_dictionary(\n",
    "    tokens=flatten_list_of_lists(tokenized_test_texts),\n",
    "    dictionary=embedding_dict)\n",
    "\n",
    "print('Out of %d train tokens, %d are in dictionary what is %d percent '\n",
    "      'of all tokens.' % (\n",
    "    all_train_tokens,\n",
    "    dictionary_train_tokens,\n",
    "    int(dictionary_train_tokens / all_train_tokens * 100),\n",
    "))\n",
    "print('Out of %d test tokens, %d are in dictionary what is %d percent '\n",
    "      'of all tokens.' % (\n",
    "    all_test_tokens,\n",
    "    dictionary_test_tokens,\n",
    "    int(dictionary_test_tokens / all_test_tokens * 100),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KmYHEgO19xnK"
   },
   "source": [
    "Now, we are almost ready to start modeling our data using Neural Networks. Before that, we need to prepare a function which will transform a sequence of words into sequence of indices as well as the embedding matrix. Your task is to write a more comprehensive implementation of `Embedder` which:\n",
    "\n",
    "- will include out of vocabulary vector and index (additional index and vector which will model out a vocabulary word). Which vector one should choose as a default value for OOW?\n",
    "- will pad sequences to a `max_len` length. If `max_len == None`, no padding should be performed.\n",
    "\n",
    "Noone said that this should be implemented from scratch. E.g., `keras` has a nice tool which might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mfs79rBQgbW8"
   },
   "outputs": [],
   "source": [
    "class Embedder:\n",
    "  \n",
    "    def get_indices_for_tokenized_texts(self, tokenized_texts):\n",
    "        return [self.get_tokens_indices(tokens) for tokens in tokenized_texts]\n",
    " \n",
    "    def get_tokens_indices(self, tokens):\n",
    "        raise NotImplementedError    \n",
    "      \n",
    "    @property\n",
    "    def embedding_dim(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @property\n",
    "    def embedding_matrix():\n",
    "        raise NotImplementedError\n",
    "      \n",
    "      \n",
    "class StudentEmbedder(Embedder):\n",
    "    # TODO\n",
    "    pass\n",
    "      \n",
    "    \n",
    "        \n",
    "embedder = Embedder(embedding_dictionary=embedding_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7bgPRZmWMBII"
   },
   "source": [
    "Now, we are ready for preparation of datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jXkm4zmqMoc_"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as torch_data\n",
    "\n",
    "\n",
    "train_set = torch_data.TensorDataset(\n",
    "    torch.LongTensor(embedder.get_indices_for_tokenized_texts(\n",
    "        tokenizer.tokenize_texts(train_texts),\n",
    "    )),\n",
    "    torch.LongTensor(train_y),\n",
    ")\n",
    "test_set = torch_data.TensorDataset(\n",
    "    torch.LongTensor(embedder.get_indices_for_tokenized_texts(\n",
    "        tokenizer.tokenize_texts(test_texts),\n",
    "    )),\n",
    "    torch.LongTensor(test_y),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uq5O1guhMJbr"
   },
   "source": [
    "... and implement  and train a model which will be a multinomial logistic regression over mean of token embeddings. \n",
    "\n",
    "Hints:\n",
    "- use `EmbeddingBag` layer,\n",
    "- is padding useful in this approach?\n",
    "\n",
    "Remember to make your model architecture as easy to change as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rOQ3KVHmDcg5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MeanEmbedding(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qy-3R2IVM417"
   },
   "source": [
    "A final model should be 1D Convolution over a sequence of vector embeddings:\n",
    "\n",
    "Hints:\n",
    "- use `Embedding` layer (beware however on its output dimensions).\n",
    "\n",
    "Extra:\n",
    "- how about making possible for your model to deal with varying length approach?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h5oYWZgqL--E"
   },
   "outputs": [],
   "source": [
    "class FullEmbedding(nn.Module):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Torch - Embeddings",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
