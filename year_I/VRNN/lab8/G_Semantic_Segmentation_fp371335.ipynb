{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "G. Semantic Segmentation [student].ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "P2F_P3bjPdaJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Semantic Segmentation\n",
        "Implement FCN-32s architecture for semantic segmentation.\n",
        "\n",
        "[Paper](https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf)"
      ]
    },
    {
      "metadata": {
        "id": "MBn4TPr3WCQh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Architecture to implement:\n",
        "\n",
        "Visualization: FCN32s.png\n",
        "\n",
        "- VGG16\n",
        "\n",
        "- Conv2D, 7x7, 4096, relu, same\n",
        "\n",
        "- Dropout 0.5\n",
        "\n",
        "- Conv2D, 1x1, 4096, relu, same\n",
        "\n",
        "- Dropout 0.5\n",
        "\n",
        "- Conv2D nb_classes 1x1\n",
        "\n",
        "- x = Conv2DTranspose(filters=nb_classes, kernel_size=(64, 64), strides=(33, 32), padding='same', activation='sigmoid')(x)\n",
        "\n",
        "- Cropping2D"
      ]
    },
    {
      "metadata": {
        "id": "saijjqlFenyJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, Dropout, Conv2DTranspose, Cropping2D\n",
        "from keras.initializers import Constant\n",
        "\n",
        "nb_classes = ...\n",
        "\n",
        "inputs = Input(shape=(360, 480, 3))\n",
        "x = VGG16(weights='imagenet', include_top=False, input_tensor=inputs)\n",
        "# ...\n",
        "model = Model(inputs=inputs, outputs=x)\n",
        "# ... disable training of first e.g. 15 layers\n",
        "model.compile(...)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W2XMKU9lc39T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !wget datasets/segmentation.npz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QNngzd3eSoIF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Data is already preprocessed for VGG16, just load it.\n",
        "data = np.load('segmentation.npz')\n",
        "train_x, train_y, test_x, test_y = data['train_x'], data['train_y'], data['test_x'], data['test_y']\n",
        "del data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cymIxl5eSoj8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(..., validation_data=(test_x, test_y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xtbKyDUbbuJt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "pred = model.predict(np.expand_dims(test_x[0], axis=0))[0].argmax(axis=2)\n",
        "plt.imshow(pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M2uea1TAudVv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.imshow(test_y[0].argmax(axis=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mrW3nTT8yQr6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Save a trained model"
      ]
    },
    {
      "metadata": {
        "id": "JTFF5itayVpK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IUdLGW5DyWeX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load the trained model and evaluate it on the test set\n",
        " - the required metric: pixel accuracy\n",
        " - additional, for those who want: mean IoU"
      ]
    },
    {
      "metadata": {
        "id": "xfjMraJmyWKY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KjWwHzXOzFts",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Submission\n",
        "You need to send:\n",
        "- the well trained model (as YourFirstName_YourSourname_Indeks.h5 file)\n",
        "- and jupyter notebook (ipynb file) with all outputs (as YourFirstName_YourSourname_Indeks.ipynb file)\n",
        "\n",
        "by the deadline.\n",
        "\n",
        "You should be able to train the model with batch_size=3. In case of problems, feel free to change Conv2D 4096->2048. Such an architecture should obtain at least 89% pixel accuracy on the test set.\n",
        "\n",
        "Please remember that typically we use different metrics for semantic segmentation."
      ]
    }
  ]
}